{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "144c6fc5",
   "metadata": {
    "tags": [
     "header"
    ]
   },
   "source": [
    "# STA 220 Assignment 2\n",
    "\n",
    "Due __Februrary 9, 2024__ by __11:59pm__. Submit your work by uploading it to Gradescope through Canvas.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Provide your solutions in new cells following each exercise description. Create as many new cells as necessary. Use code cells for your Python scripts and Markdown cells for explanatory text or answers to non-coding questions. Answer all textual questions in complete sentences.\n",
    "2. The use of assistive tools is permitted, but must be indicated. You will be graded on you proficiency in coding. Produce high quality code by adhering to proper programming principles. \n",
    "3. Export the .jpynb as .pdf and submit it on Gradescope in time. To facilitate grading, indicate the area of the solution on the submission. Submissions without indication will be marked down. No late submissions accepted. \n",
    "4. If test cases are given, your solution must be in the same format. \n",
    "5. The total number of points is 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe607f59",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__Exercise 1__\n",
    "\n",
    "We will compute the [PageRank](https://en.wikipedia.org/wiki/PageRank) of the articles of the [Sinhala](https://en.wikipedia.org/wiki/Sinhala_language) wikipedia, which is available at [si.wikipedia.org](https://si.wikipedia.org/wiki/%E0%B6%B8%E0%B7%94%E0%B6%BD%E0%B7%8A_%E0%B6%B4%E0%B7%92%E0%B6%A7%E0%B7%94%E0%B7%80). Additional information of the Sinhala wiki can be found [here](https://meta.wikimedia.org/wiki/List_of_Wikipedias). \n",
    "\n",
    "_Hints: If you don't speak Sinhalese, you might want to learn the wiki logic from the english wikipedia, and translate your findings. Also, caching is highly recommended._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276cd17",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(a)__ Use the special [AllPages](https://si.wikipedia.org/wiki/%E0%B7%80%E0%B7%92%E0%B7%81%E0%B7%9A%E0%B7%82:%E0%B7%83%E0%B7%92%E0%B6%BA%E0%B7%85%E0%B7%94_%E0%B6%B4%E0%B7%92%E0%B6%A7%E0%B7%94) page and understand its logic to retrieve the url of all articles in the sinhalese wikipedia. Make sure to skip redirections.\n",
    "\n",
    "_How many articles are there?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db942f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4f6954f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST CODE ON ENGLISH ALL PAGE\n",
    "\"\"\"#starting url\n",
    "url = \"https://en.wikipedia.org/wiki/Special:AllPages\"\n",
    "#base wikipedia url\n",
    "base = \"https://en.wikipedia.org\"\n",
    "#get the response\n",
    "response = requests.get(url)\n",
    "bs = BeautifulSoup(response.text)\n",
    "#hard code the next url\n",
    "\n",
    "\n",
    "url_list = []\n",
    "redirect_list = []\n",
    "#First page\n",
    "for link in bs.find('ul', {'class':'mw-allpages-chunk'}).find_all('a', {'href':re.compile(\"^/wiki\")}):\n",
    "    if link.get('class') == ['mw-redirect']:\n",
    "        redirect_list.append(base + link.get('href'))\n",
    "    else:\n",
    "        full_url = base + link.get('href')\n",
    "        url_list.append(full_url)\n",
    "ref_list = bs.find('div', {'class':'mw-allpages-nav'}).find_all('a', {'href':re.compile(\"^/w\")})\n",
    "if len(ref_list) == 1:\n",
    "    next_url = base + ref_list[0].get('href')\n",
    "else:\n",
    "    next_url = base + ref_list[1].get('href')\n",
    "\n",
    "\n",
    "#Begin while loop and stops when the navigate has only 1 element\n",
    "response = requests.get(next_url)\n",
    "ref_list = [0,1]\n",
    "i = 0\n",
    "while len(ref_list) > 1 and i < 5:\n",
    "    bs = BeautifulSoup(response.text)\n",
    "    ref_list = bs.find('div', {'class':'mw-allpages-nav'}).find_all('a', {'href':re.compile(\"^/w\")})\n",
    "    if len(ref_list) == 1:\n",
    "        next_url = base + ref_list[0].get('href')\n",
    "    else:\n",
    "        next_url = base + ref_list[1].get('href')\n",
    "    for link in bs.find('ul', {'class':'mw-allpages-chunk'}).find_all('a', {'href':re.compile(\"^/wiki\")}):\n",
    "        if link.get('class') == ['mw-redirect']:\n",
    "            redirect_list.append(base + link.get('href'))\n",
    "        else:\n",
    "            full_url = base + link.get('href')\n",
    "            url_list.append(full_url)\n",
    "    i += 1 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26888655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS THE CODE TO GET SINHALA URL FROM ALL PAGES\n",
    "\"\"\"url = \"https://si.wikipedia.org/wiki/%E0%B7%80%E0%B7%92%E0%B7%81%E0%B7%9A%E0%B7%82:%E0%B7%83%E0%B7%92%E0%B6%BA%E0%B7%85%E0%B7%94_%E0%B6%B4%E0%B7%92%E0%B6%A7%E0%B7%94\"\n",
    "#base wikipedia url\n",
    "base = \"https://si.wikipedia.org\"\n",
    "#get the response\n",
    "response = requests.get(url)\n",
    "bs = BeautifulSoup(response.text)\n",
    "#hard code the next url\n",
    "\n",
    "\n",
    "url_list = []\n",
    "redirect_list = []\n",
    "#First page\n",
    "for link in bs.find('ul', {'class':'mw-allpages-chunk'}).find_all('a', {'href':re.compile(\"^/wiki\")}):\n",
    "    if link.get('class') == ['mw-redirect']:\n",
    "        redirect_list.append(base + link.get('href'))\n",
    "    else:\n",
    "        full_url = base + link.get('href')\n",
    "        url_list.append(full_url)\n",
    "ref_list = bs.find('div', {'class':'mw-allpages-nav'}).find_all('a', {'href':re.compile(\"^/w\")})\n",
    "if len(ref_list) == 1:\n",
    "    next_url = base + ref_list[0].get('href')\n",
    "else:\n",
    "    next_url = base + ref_list[1].get('href')\n",
    "\n",
    "\n",
    "#Begin while loop and stops when the navigate has only 1 element\n",
    "response = requests.get(next_url)\n",
    "ref_list = [0,1]\n",
    "i = 0\n",
    "while len(ref_list) > 1:\n",
    "    bs = BeautifulSoup(response.text)\n",
    "    ref_list = bs.find('div', {'class':'mw-allpages-nav'}).find_all('a', {'href':re.compile(\"^/w\")})\n",
    "    if len(ref_list) == 1:\n",
    "        next_url = base + ref_list[0].get('href')\n",
    "    else:\n",
    "        next_url = base + ref_list[1].get('href')\n",
    "    for link in bs.find('ul', {'class':'mw-allpages-chunk'}).find_all('a', {'href':re.compile(\"^/wiki\")}):\n",
    "        if link.get('class') == ['mw-redirect']:\n",
    "            redirect_list.append(base + link.get('href'))\n",
    "        else:\n",
    "            full_url = base + link.get('href')\n",
    "            url_list.append(full_url)\n",
    "    response = requests.get(next_url)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6159f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save list of links to a txt file for easy access later\n",
    "\"\"\"with open('../Data/links.txt', 'w') as f:\n",
    "    for link in url_list:\n",
    "        f.write(f\"{link}\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df11f7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24234"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read URL List and save to list\n",
    "with open('../Data/links.txt') as f:\n",
    "    url_list = f.read().splitlines()\n",
    "\n",
    "len(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90213dd",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(b, i)__ Scan all articles in the sinhalese wikipedia and retrieve all links to other articles. Avoid links to special pages, images or the ones that point to another website. Only count the proper article for links that point to a specific section. Use regular expressions to manage these cases. \n",
    "__(ii)__ Make sure to match redirections to their correct destiation article. To this end, find how wikipedia treats redirections and retrieve the true article. _(Help: Try searching for 'uc davis' on en.wikipedia.org')_\n",
    "__(iii)__ Use threading to request all articles and obtain all links to other articles. _(Attention: This takes about thirty minutes!)_\n",
    "\n",
    "\n",
    "_How many links to other articles are there?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5bdd68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickkle for easy dictionary save\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89932553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function that takes a URL and traverses through the article body and saves every link\n",
    "def url_traversal(url):\n",
    "    base = \"https://si.wikipedia.org\"\n",
    "    url_dictionary = {}\n",
    "    response = requests.get(url)\n",
    "    bs = BeautifulSoup(response.text)\n",
    "    summary = bs.find('div', {'class':'mw-content-ltr mw-parser-output'}).find_all('p')\n",
    "    test_list = []\n",
    "    if len(summary) > 1:\n",
    "        for paragraph in summary[1:]:\n",
    "            links_list = paragraph.find_all('a',{'href':re.compile(\"/w\")})\n",
    "            for link in links_list:\n",
    "                if link.get('class') == ['mw-redirect']:\n",
    "                    redirect_url = base + link.get('href')\n",
    "                    response_loop = requests.get(redirect_url).text\n",
    "                    bs_loop = BeautifulSoup(response_loop)\n",
    "                    test_list.append(bs_loop.find('link', {'rel':'canonical'}).get('href'))\n",
    "                else:\n",
    "                    test_list.append(base + link.get('href'))\n",
    "    elif len(summary) == 1:\n",
    "        links_list = summary[0].find_all('a',{'href':re.compile(\"/w\")})\n",
    "        for link in links_list:\n",
    "            if link.get('class') == ['mw-redirect']:\n",
    "                redirect_url = base + link.get('href')\n",
    "                response_loop = requests.get(redirect_url).text\n",
    "                bs_loop = BeautifulSoup(response_loop)\n",
    "                test_list.append(bs_loop.find('link', {'rel':'canonical'}).get('href'))\n",
    "            else:\n",
    "                test_list.append(base + link.get('href'))\n",
    "    else:\n",
    "        pass\n",
    "    url_dictionary[url] = test_list\n",
    "    return url_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bc3faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use threads to speed up this process this definition takes list of url and goes through all of them and updates it into a dictionary\n",
    "import concurrent.futures\n",
    "def threaded_traversal(url_dictionary, url_list):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = 16) as executor:\n",
    "        for result in executor.map(url_traversal, url_list):\n",
    "            url_dictionary.update(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7725334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty dictionary and save a dictionary of links to other articles\n",
    "url_dictionary = {}\n",
    "threaded_traversal(url_dictionary, url_list[20000:len(url_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f84613a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves the dictionary into another file for ease of access in the future\n",
    "with open(\"../Data/url_dict.pkl\", \"ab\") as f:\n",
    "    pickle.dump(url_dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1550e54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24234"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the dictionary file and save it into a dictionary\n",
    "d = {}\n",
    "with open(\"../Data/url_dict.pkl\", \"rb\") as f:\n",
    "    while True:\n",
    "        try:\n",
    "            a = pickle.load(f)\n",
    "        except EOFError:\n",
    "            break\n",
    "        else:\n",
    "            d.update(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28f55378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148237\n"
     ]
    }
   ],
   "source": [
    "#save the number of articles that are referenced\n",
    "total = 0\n",
    "for url in url_list:\n",
    "    total += len(d[url])\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775e365",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(c)__ Compute the transition matrix (see [here](https://en.wikipedia.org/wiki/Google_matrix) and [here](https://www.amsi.org.au/teacher_modules/pdfs/Maths_delivers/Pagerank5.pdf) for step-by-step instructions). Make sure to tread dangling nodes. You may want to use: \n",
    "```\n",
    "from scipy.sparse import csr_matrix\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be68fe99",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(d, i)__ Set the damping factor to `0.85` and compute the PageRank for each article, using fourty iterations and starting with a vector with equal entries. __(ii)__ Obtain the top ten articles in terms of PageRank, and, retrieving the articles again, find the correponding English article, if available. \n",
    "\n",
    "_Return the corresponding English article titles of the top ten articles from the Sinhalese wikipedia._"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
